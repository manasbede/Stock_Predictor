{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPwtfMOgwE97"
      },
      "source": [
        "# Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uRUIUBPwKYK"
      },
      "source": [
        "## Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-0kDw4GwHVa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "!pip install pymongo\n",
        "import pymongo\n",
        "from itertools import islice\n",
        "from datetime import datetime, timedelta\n",
        "from bson.objectid import ObjectId\n",
        "import yfinance as yf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlH3T-FWyXQV"
      },
      "source": [
        "## Data Scraping\n",
        "Scraping Financial Reports Company-wise from QuickFS and Storing on MongoDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7-7LQXqyaTz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "!pip install pymongo\n",
        "import pymongo\n",
        "\n",
        "symbol_df=pd.read_csv('/content/drive/MyDrive/<base_path>/Dataset/nasdaq_screener.csv')\n",
        "symbol_df['DBID']=\"\"\n",
        "\n",
        "client = pymongo.MongoClient(\"mongodb+srv://<your_instance_to_store_data>\")\n",
        "\n",
        "for index, row in symbol_df.iterrows():\n",
        "    if \"^\" not in str(row['Symbol']):\n",
        "      url= f\"https://public-api.quickfs.net/v1/data/all-data/{row['Symbol']}?api_key=<your_token>\"\n",
        "      print(url)\n",
        "      try:\n",
        "        response = requests.get(url)\n",
        "        data=response.json()\n",
        "        db = client.NASDAQ_DATA\n",
        "        mycol = db.NASDAQ\n",
        "\n",
        "        x = mycol.insert_one(data)\n",
        "        #print(x)\n",
        "        symbol_df.at[index, 'DBID'] = x.inserted_id\n",
        "      except:\n",
        "        print(\"Issue with URL ignoring this\")\n",
        "\n",
        "\n",
        "symbol_df.to_csv('/content/drive/MyDrive/<base_path>/Dataset/nasdaq_screener.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR8-wOUvzdl5"
      },
      "source": [
        "## Creating CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KpxS_iszf67"
      },
      "outputs": [],
      "source": [
        "main=pd.DataFrame(columns=['Symbol',\n",
        " 'period_end_date',\n",
        " 'revenue',\n",
        " 'cogs',\n",
        " 'gross_profit',\n",
        " 'sga',\n",
        " 'rnd',\n",
        " 'special_charges',\n",
        " 'other_opex',\n",
        " 'total_opex',\n",
        " 'operating_income',\n",
        " 'interest_income',\n",
        " 'interest_expense',\n",
        " 'net_interest_income_normal',\n",
        " 'other_nonoperating_income',\n",
        " 'pretax_income',\n",
        " 'income_tax',\n",
        " 'net_income_continuing',\n",
        " 'net_income_discontinued',\n",
        " 'income_allocated_to_minority_interest',\n",
        " 'other_income_statement_items',\n",
        " 'net_income',\n",
        " 'preferred_dividends',\n",
        " 'net_income_available_to_shareholders',\n",
        " 'eps_basic',\n",
        " 'eps_diluted',\n",
        " 'shares_basic',\n",
        " 'shares_diluted',\n",
        " 'da_income_statement_supplemental',\n",
        " 'cash_and_equiv',\n",
        " 'st_investments',\n",
        " 'receivables',\n",
        " 'inventories',\n",
        " 'other_current_assets',\n",
        " 'total_current_assets',\n",
        " 'equity_and_other_investments',\n",
        " 'ppe_gross',\n",
        " 'accumulated_depreciation',\n",
        " 'ppe_net',\n",
        " 'intangible_assets',\n",
        " 'goodwill',\n",
        " 'other_lt_assets',\n",
        " 'total_assets',\n",
        " 'accounts_payable',\n",
        " 'tax_payable',\n",
        " 'current_accrued_liabilities',\n",
        " 'st_debt',\n",
        " 'current_capital_leases',\n",
        " 'current_deferred_revenue',\n",
        " 'current_deferred_tax_liability',\n",
        " 'other_current_liabilities',\n",
        " 'total_current_liabilities',\n",
        " 'lt_debt',\n",
        " 'noncurrent_capital_leases',\n",
        " 'pension_liabilities',\n",
        " 'noncurrent_deferred_revenue',\n",
        " 'other_lt_liabilities',\n",
        " 'total_liabilities',\n",
        " 'common_stock',\n",
        " 'preferred_stock',\n",
        " 'retained_earnings',\n",
        " 'aoci',\n",
        " 'apic',\n",
        " 'treasury_stock',\n",
        " 'minority_interest_liability',\n",
        " 'other_equity',\n",
        " 'total_equity',\n",
        " 'total_liabilities_and_equity',\n",
        " 'total_noncurrent_assets',\n",
        " 'total_noncurrent_liabilities',\n",
        " 'cfo_net_income',\n",
        " 'cfo_da',\n",
        " 'cfo_receivables',\n",
        " 'cfo_inventory',\n",
        " 'cfo_prepaid_expenses',\n",
        " 'cfo_other_working_capital',\n",
        " 'cfo_change_in_working_capital',\n",
        " 'cfo_deferred_tax',\n",
        " 'cfo_stock_comp',\n",
        " 'cfo_other_noncash_items',\n",
        " 'cf_cfo',\n",
        " 'cfi_ppe_purchases',\n",
        " 'cfi_ppe_sales',\n",
        " 'cfi_ppe_net',\n",
        " 'cfi_acquisitions',\n",
        " 'cfi_divestitures',\n",
        " 'cfi_acquisitions_net',\n",
        " 'cfi_investment_purchases',\n",
        " 'cfi_investment_sales',\n",
        " 'cfi_investment_net',\n",
        " 'cfi_intangibles_net',\n",
        " 'cfi_other',\n",
        " 'cf_cfi',\n",
        " 'cff_common_stock_issued',\n",
        " 'cff_common_stock_repurchased',\n",
        " 'cff_common_stock_net',\n",
        " 'cff_pfd_net',\n",
        " 'cff_debt_net',\n",
        " 'cff_dividend_paid',\n",
        " 'cff_other',\n",
        " 'cf_cff',\n",
        " 'cf_forex',\n",
        " 'cf_net_change_in_cash',\n",
        " 'capex',\n",
        " 'cff_pfd_issued',\n",
        " 'cff_pfd_repurchased',\n",
        " 'cff_debt_issued',\n",
        " 'cff_debt_repaid',\n",
        " 'ebitda',\n",
        " 'fcf',\n",
        " 'income_tax_rate',\n",
        " 'nopat',\n",
        " 'book_value',\n",
        " 'tangible_book_value',\n",
        " 'roa',\n",
        " 'roe',\n",
        " 'roic',\n",
        " 'roic_legacy',\n",
        " 'roce',\n",
        " 'rotce',\n",
        " 'gross_margin',\n",
        " 'ebitda_margin',\n",
        " 'operating_margin',\n",
        " 'pretax_margin',\n",
        " 'net_income_margin',\n",
        " 'fcf_margin',\n",
        " 'assets_to_equity',\n",
        " 'equity_to_assets',\n",
        " 'debt_to_equity',\n",
        " 'debt_to_assets',\n",
        " 'revenue_per_share',\n",
        " 'ebitda_per_share',\n",
        " 'operating_income_per_share',\n",
        " 'pretax_income_per_share',\n",
        " 'fcf_per_share',\n",
        " 'book_value_per_share',\n",
        " 'tangible_book_per_share',\n",
        " 'market_cap',\n",
        " 'enterprise_value',\n",
        " 'price_to_earnings',\n",
        " 'price_to_book',\n",
        " 'price_to_tangible_book',\n",
        " 'price_to_sales',\n",
        " 'price_to_fcf',\n",
        " 'price_to_pretax_income',\n",
        " 'enterprise_value_to_earnings',\n",
        " 'enterprise_value_to_book',\n",
        " 'enterprise_value_to_tangible_book',\n",
        " 'enterprise_value_to_sales',\n",
        " 'enterprise_value_to_fcf',\n",
        " 'enterprise_value_to_pretax_income',\n",
        " 'revenue_growth',\n",
        " 'gross_profit_growth',\n",
        " 'ebitda_growth',\n",
        " 'operating_income_growth',\n",
        " 'pretax_income_growth',\n",
        " 'net_income_growth',\n",
        " 'eps_diluted_growth',\n",
        " 'shares_diluted_growth',\n",
        " 'cash_and_equiv_growth',\n",
        " 'ppe_growth',\n",
        " 'total_assets_growth',\n",
        " 'total_equity_growth',\n",
        " 'cfo_growth',\n",
        " 'capex_growth',\n",
        " 'fcf_growth',\n",
        " 'dividends_per_share_growth',\n",
        " 'shares_eop_growth',\n",
        " 'revenue_cagr_10',\n",
        " 'eps_diluted_cagr_10',\n",
        " 'total_assets_cagr_10',\n",
        " 'total_equity_cagr_10',\n",
        " 'fcf_cagr_10',\n",
        " 'dividends_per_share_cagr_10',\n",
        " 'cf_cfo_cagr_10',\n",
        " 'current_ratio',\n",
        " 'payout_ratio',\n",
        " 'peg_ratio',\n",
        " 'shares_eop_change',\n",
        " 'net_debt',\n",
        " 'shares_eop',\n",
        " 'dividends',\n",
        " 'original_filing_date',\n",
        " 'period_end_price',\n",
        " 'gross_margin_median',\n",
        " 'pretax_margin_median',\n",
        " 'operating_income_margin_median',\n",
        " 'fcf_margin_median',\n",
        " 'roa_median',\n",
        " 'roe_median',\n",
        " 'roic_median',\n",
        " 'assets_to_equity_median',\n",
        " 'debt_to_assets_median',\n",
        " 'debt_to_equity_median',\n",
        " 'roic_5yr_avg',\n",
        " 'fiscal_quarter_number',\n",
        " 'fiscal_quarter_key',\n",
        " 'Average_Stock_Value_After_Result'])\n",
        "\n",
        "\n",
        "def getTechnicalData(ticker,start_date):\n",
        "    start_date = datetime.strptime(start_date, '%Y-%m')\n",
        "    start_date = str(start_date.strftime('%Y-%m-%d'))\n",
        "    #print(ticker, start_data)\n",
        "    try:\n",
        "      stock_data = yf.download(ticker, start=start_date, end=\"2023-12-31\")\n",
        "      #print(stock_data)\n",
        "      return (stock_data)\n",
        "    except:\n",
        "      return None\n",
        "counter = 0\n",
        "for index, row in symbol_df.iterrows():\n",
        "  if \"^\" not in str(row['Symbol']):\n",
        "      url= f\"https://public-api.quickfs.net/v1/data/all-data/{row['Symbol']}?api_key=<token>\"\n",
        "      print(url)\n",
        "      try:\n",
        "        response = requests.get(url)\n",
        "        data=response.json()\n",
        "        tmp_fund_data=pd.DataFrame(data['data']['financials']['quarterly'])\n",
        "        tmp_tech_data = getTechnicalData(row['Symbol'],tmp_fund_data['period_end_date'][0])\n",
        "        tmp_fund_data.insert(0,\"Symbol\",row['Symbol'])\n",
        "        tmp_fund_data['Average_Stock_Value_After_Result'] = ''\n",
        "        for index, row in tmp_fund_data.iterrows():\n",
        "          start_data = row['period_end_date']\n",
        "          end_date = datetime.strptime(start_data, \"%Y-%m\") + timedelta(30 * 4)\n",
        "          end_date = str(end_date.strftime('%Y-%m-%d'))\n",
        "          start_data = datetime.strptime(start_data, \"%Y-%m\") + timedelta(30 * 1)\n",
        "          start_data = str(start_data.strftime('%Y-%m-%d'))\n",
        "\n",
        "          tmp=tmp_tech_data.loc[(tmp_tech_data.index>=start_data) & (tmp_tech_data.index<=end_date)]\n",
        "\n",
        "          tmp_fund_data.at[index, 'Average_Stock_Value_After_Result'] = (tmp['Close'].mean())\n",
        "\n",
        "        main=pd.concat([main, tmp_fund_data], axis=0)\n",
        "        main.reset_index(drop=True,inplace=True)\n",
        "        counter+=1\n",
        "        if (counter%100==0):\n",
        "          counter = 0\n",
        "          main.to_csv('/content/drive/MyDrive/<base_path>/Dataset/main.csv',index=False)\n",
        "      except:\n",
        "        print(\"Issue with URL ignoring this\")\n",
        "main.to_csv('/content/drive/MyDrive/<base_path>/Dataset/main.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYANKZx7ziBN"
      },
      "outputs": [],
      "source": [
        "def remove_single_value_columns(df):\n",
        "    for col in df.columns:\n",
        "        if df[col].nunique() == 1:\n",
        "            df.drop(col, axis=1, inplace=True)\n",
        "\n",
        "    return (df)\n",
        "\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/<base_path>/Dataset/main.csv')\n",
        "data = data.drop(['Average_Stock_Value_After_Result'])\n",
        "# Compute the correlation matrix\n",
        "corr_matrix = data.corr().abs()\n",
        "\n",
        "# Set a threshold for highly correlated features\n",
        "threshold = 0.8\n",
        "\n",
        "# Find index of feature columns with correlation greater than threshold\n",
        "high_corr_cols = np.where(corr_matrix > threshold)\n",
        "\n",
        "# Create a set to store pairs of correlated features\n",
        "correlated_features = set()\n",
        "\n",
        "# Iterate through columns and identify pairs of highly correlated features\n",
        "for i in range(len(high_corr_cols[0])):\n",
        "    # Exclude diagonal and lower triangular elements\n",
        "    if high_corr_cols[0][i] != high_corr_cols[1][i] and high_corr_cols[0][i] < high_corr_cols[1][i]:\n",
        "        # Add the pair of feature names to the set\n",
        "        correlated_features.add((data.columns[high_corr_cols[0][i]], data.columns[high_corr_cols[1][i]]))\n",
        "\n",
        "# Print the correlated feature pairs\n",
        "print(\"Highly correlated feature pairs:\")\n",
        "for pair in correlated_features:\n",
        "    print(pair)\n",
        "\n",
        "# Drop one feature from each correlated pair\n",
        "for feature1, feature2 in correlated_features:\n",
        "    # Drop feature2 (you can choose feature1 instead if you prefer)\n",
        "    data.drop(columns=[feature2], inplace=True)\n",
        "\n",
        "# Print the remaining features after dropping correlated ones\n",
        "print(\"\\nRemaining features after dropping highly correlated features:\")\n",
        "print(data.columns)\n",
        "\n",
        "data=remove_single_value_columns(data)\n",
        "data.dropna(axis=1, how=\"all\")\n",
        "\n",
        "data.to_csv('/content/drive/MyDrive/<base_path>/Dataset/main.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
