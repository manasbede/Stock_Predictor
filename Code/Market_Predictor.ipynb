{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IIBpo-gdLVJ"
      },
      "source": [
        "Required Files at Working Directory\n",
        "\n",
        "\n",
        "1.   main.csv\n",
        "2.   nasdaq_screener.csv\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGqjvbxSUDmu"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwqobcKvT_ot"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error,mean_absolute_percentage_error\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96xhX_0mUPBJ"
      },
      "source": [
        "# Set Working Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNo7VoK-US1o"
      },
      "outputs": [],
      "source": [
        "work_dir=\"/content/drive/MyDrive/<base_path>/Dataset\"\n",
        "result_dir=\"/content/drive/MyDrive/<base_path>/Results\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F43LdsZ8UJlE"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fA_OHgtULcd",
        "outputId": "59835c59-a079-4c03-88fa-88a91bf569e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of companies with more than 80 samples: 1658\n",
            "109\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv(f\"{work_dir}/main.csv\")\n",
        "cols_to_drop = data.columns[data.columns.str.contains('fiscal_quarter_key')]\n",
        "data.drop(cols_to_drop, axis=1, inplace=True)\n",
        "grouped = data.groupby('Symbol')\n",
        "\n",
        "# Count the number of samples per symbol\n",
        "lengths = [(symbol, len(group)) for symbol, group in grouped]\n",
        "lengths_sorted = sorted(lengths, key=lambda x: x[1])\n",
        "\n",
        "# Count the number of companies with more than or equal to 80 samples\n",
        "count = sum(1 for symbol, length in lengths_sorted if length >= 80)\n",
        "print(\"Number of companies with more than 80 samples:\", count)\n",
        "stockno = count\n",
        "# Filter subgroups with more than or equal to 80 samples\n",
        "filtered_groups = [(symbol, group) for symbol, group in grouped if len(group) >=80]\n",
        "\n",
        "# Define a function to truncate each subgroup to 80 samples\n",
        "def truncate_to_80(group):\n",
        "    return group.iloc[:80]\n",
        "\n",
        "# Apply the truncation function to each subgroup in filtered_groups\n",
        "truncated_groups = [(symbol, truncate_to_80(group)) for symbol, group in filtered_groups]\n",
        "filtered_data = pd.concat([group for symbol, group in truncated_groups])\n",
        "companies = filtered_data['Symbol'].unique()\n",
        "cols_to_drop = data.columns[data.columns.str.contains('Symbol')]\n",
        "filtered_data.drop(cols_to_drop, axis=1, inplace=True)\n",
        "cols_to_drop = data.columns[data.columns.str.contains('period_end_date')]\n",
        "filtered_data.drop(cols_to_drop, axis=1, inplace=True)\n",
        "cols_to_drop = data.columns[data.columns.str.contains('fiscal_quarter_number')]\n",
        "filtered_data.drop(cols_to_drop, axis=1, inplace=True)\n",
        "data = filtered_data.to_numpy()\n",
        "print(len(data[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD89p_PWUXHm"
      },
      "source": [
        "# Pre Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBn3bk7gUZ5A"
      },
      "outputs": [],
      "source": [
        "def pre_process(data,sequence_length,overlap,batch_size):\n",
        "  # here 70 samples are grouped together into a sub group : Train and 10 : Test\n",
        "  grouped_data = []\n",
        "  grouped_data_test = []\n",
        "  for i in range(stockno):\n",
        "      train_start_index = i*80\n",
        "      train_end_index = (i+1)*80-10\n",
        "\n",
        "      test_start_index = train_end_index\n",
        "      test_end_index = test_start_index + 10\n",
        "\n",
        "      grouped_data.append(data[train_start_index:train_end_index])\n",
        "      grouped_data_test.append(data[test_start_index:test_end_index])\n",
        "  grouped_data = np.array(grouped_data) # contains data for each company in subgroup\n",
        "  len(grouped_data[-1])\n",
        "  grouped_data[0][0:0+sequence_length]\n",
        "\n",
        "  # this cell normnalize the dataset by groupwise\n",
        "  train_mean_ref=[]\n",
        "  train_std_ref=[]\n",
        "\n",
        "  test_mean_ref=[]\n",
        "  test_std_ref=[]\n",
        "\n",
        "  for i in range(len(grouped_data)):\n",
        "      mean_vector = np.mean(grouped_data[i], axis=0)\n",
        "      std_vector = np.std(grouped_data[i], axis=0)\n",
        "\n",
        "      # Check for zero standard deviation\n",
        "      zero_std_indices = np.where(std_vector == 0)[0]\n",
        "\n",
        "      # Avoid division by zero and handle NaN\n",
        "      std_vector[std_vector == 0] = 1  # Replace zero standard deviations with 1 to avoid division by zero\n",
        "\n",
        "      # Reference of mean and std\n",
        "      train_mean_ref.append(mean_vector)\n",
        "      train_std_ref.append(std_vector)\n",
        "      # Normalize the data\n",
        "      normalized_data = (grouped_data[i] - mean_vector) / std_vector\n",
        "\n",
        "      # Handle elements where standard deviation was zero\n",
        "      normalized_data[:, zero_std_indices] = 0  # Set corresponding elements to zero\n",
        "\n",
        "      # Assign the normalized data back to grouped_data\n",
        "      grouped_data[i] = normalized_data\n",
        "\n",
        "\n",
        "\n",
        "      normalized_data = (grouped_data_test[i] - mean_vector) / std_vector\n",
        "\n",
        "      # Handle elements where standard deviation was zero\n",
        "      normalized_data[:, zero_std_indices] = 0  # Set corresponding elements to zero\n",
        "\n",
        "      # Assign the normalized data back to grouped_data\n",
        "      grouped_data_test[i] = normalized_data\n",
        "\n",
        "\n",
        "  # for i in range(len(grouped_data_test)):\n",
        "  #     mean_vector = np.mean(grouped_data_test[i], axis=0)\n",
        "  #     std_vector = np.std(grouped_data_test[i], axis=0)\n",
        "\n",
        "  #     # Check for zero standard deviation\n",
        "  #     zero_std_indices = np.where(std_vector == 0)[0]\n",
        "\n",
        "  #     # Avoid division by zero and handle NaN\n",
        "  #     std_vector[std_vector == 0] = 1  # Replace zero standard deviations with 1 to avoid division by zero\n",
        "\n",
        "  #     # Reference of mean and std\n",
        "  #     test_mean_ref.append(mean_vector)\n",
        "  #     test_std_ref.append(std_vector)\n",
        "  #     # Normalize the data\n",
        "  #     normalized_data = (grouped_data_test[i] - mean_vector) / std_vector\n",
        "\n",
        "  #     # Handle elements where standard deviation was zero\n",
        "  #     normalized_data[:, zero_std_indices] = 0  # Set corresponding elements to zero\n",
        "\n",
        "  #     # Assign the normalized data back to grouped_data\n",
        "  #     grouped_data_test[i] = normalized_data\n",
        "\n",
        "\n",
        "\n",
        "  dataset=[]\n",
        "  for i in range(len(grouped_data)):#1658\n",
        "      for j in range(len(grouped_data[i])):#70\n",
        "          if j*(sequence_length - overlap)+sequence_length < len(grouped_data[i]):\n",
        "              datapoint = grouped_data[i][j*(sequence_length - overlap):j*(sequence_length - overlap)+sequence_length]\n",
        "              #print(len(datapoint))\n",
        "              dataset.append(datapoint)\n",
        "          else:\n",
        "              break\n",
        "  dataset_test=[]\n",
        "  for i in range(len(grouped_data_test)):#1658\n",
        "      for j in range(len(grouped_data_test[i])):#70\n",
        "          if j*(sequence_length - overlap)+sequence_length < len(grouped_data_test[i]):\n",
        "              datapoint = grouped_data_test[i][j*(sequence_length - overlap):j*(sequence_length - overlap)+sequence_length]\n",
        "              dataset_test.append(datapoint)\n",
        "          else:\n",
        "              break\n",
        "\n",
        "\n",
        "\n",
        "  dataset = np.array(dataset)\n",
        "  features = dataset[:, :, :-1]\n",
        "\n",
        "  dataset_test = np.array(dataset_test)\n",
        "  features_test = dataset_test[:, :, :-1]\n",
        "  print(features.shape, features_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "  targets = []\n",
        "  for i in range(len(grouped_data)):#1658\n",
        "      for j in range(len(grouped_data[i])):#70\n",
        "          if j*(sequence_length - overlap)+sequence_length < len(grouped_data[i]):\n",
        "              target = grouped_data[i][j*(sequence_length - overlap)+sequence_length][-1]\n",
        "              targets.append(target)\n",
        "          else:\n",
        "              break\n",
        "\n",
        "  targets_test = []\n",
        "  for i in range(len(grouped_data_test)):#1658\n",
        "      for j in range(len(grouped_data_test[i])):#10\n",
        "          if j*(sequence_length - overlap)+sequence_length < len(grouped_data_test[i]):\n",
        "              target = grouped_data_test[i][j*(sequence_length - overlap)+sequence_length][-1]\n",
        "\n",
        "              targets_test.append(target)\n",
        "          else:\n",
        "              #print(j)\n",
        "              break\n",
        "\n",
        "  targets = np.array(targets)\n",
        "  print(targets.shape)\n",
        "  features = torch.tensor(features, dtype=torch.float32)\n",
        "  targets = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "\n",
        "  targets_test = np.array(targets_test)\n",
        "  print(targets_test.shape)\n",
        "  features_test = torch.tensor(features_test, dtype=torch.float32)\n",
        "  targets_test = torch.tensor(targets_test, dtype=torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "  X_train, X_val, y_train, y_val = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
        "  train_dataset = TensorDataset(X_train, y_train)\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  val_dataset = TensorDataset(X_val, y_val)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  test_dataset = TensorDataset(features_test, targets_test)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader,val_loader,test_loader,train_mean_ref,train_std_ref"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH3nuEeyVnAi"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7-KgnbqVtBq"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIfBHQeOVsP2"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, output_size, sequence_length):\n",
        "        super(MLP, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(in_features=(input_size*sequence_length), out_features=256)\n",
        "        self.dropout1 = nn.Dropout(p=0.1)\n",
        "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
        "        self.dropout2 = nn.Dropout(p=0.1)\n",
        "        self.fc3 = nn.Linear(in_features=128, out_features=64)\n",
        "        self.dropout3 = nn.Dropout(p=0.1)\n",
        "        self.fc4 = nn.Linear(in_features=64, out_features=32)\n",
        "        self.dropout4 = nn.Dropout(p=0.1)\n",
        "        self.fc5 = nn.Linear(in_features=32, out_features=16)\n",
        "        self.fc6 = nn.Linear(in_features=16, out_features=output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.dropout3(x)\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = self.dropout4(x)\n",
        "        x = F.relu(self.fc5(x))\n",
        "        x = self.fc6(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm6t-0A8WSxQ"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO8yapwEWZuf"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "        c_0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "        _, (h_n, _) = self.lstm(x, (h_0, c_0))\n",
        "        output = self.fc(h_n[-1])\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDYIbzxvWj0W"
      },
      "source": [
        "## LSTM-AM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45W9YjWfWlIJ"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "class LSTMAttentionRegressor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTMAttentionRegressor, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.attention = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "        c_0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
        "        output, _ = self.lstm(x, (h_0, c_0))\n",
        "        attention_weights = nn.functional.softmax(self.attention(output), dim=1)\n",
        "        context_vector = torch.sum(attention_weights * output, dim=1)\n",
        "        output = self.fc(context_vector)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNBUgpp2Q8YQ"
      },
      "source": [
        "## BiLSTM-AM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLJMYy5iRA5K"
      },
      "outputs": [],
      "source": [
        "class BiLSTM_AM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, sequence_length, dropout_prob):\n",
        "        super(BiLSTM_AM, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True, bias=True)\n",
        "        self.lstm_2 = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True, bias=True)\n",
        "        self.attention_linear = nn.Linear(hidden_dim * 2, 1, bias=True)\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim, bias=True)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (batch_size, sequence_length, input_dim)\n",
        "        batch_size, sequence_length, input_dim = x.size()\n",
        "\n",
        "        # LSTM layers\n",
        "        x, _ = self.lstm_1(x)\n",
        "        x, _ = self.lstm_2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Attention mechanism\n",
        "        attention_weights = F.softmax(self.attention_linear(x), dim=1)\n",
        "        attended_out = torch.sum(attention_weights * x, dim=1)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.linear(attended_out)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT9N_Ah8WuKU"
      },
      "source": [
        "## CNN-BiLSTM-AT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8hqb6-3W8Wd"
      },
      "outputs": [],
      "source": [
        "#Model Implementation\n",
        "\n",
        "class CNN_BiLSTM_AM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, sequence_length, dropout_prob):\n",
        "        super(CNN_BiLSTM_AM, self).__init__()\n",
        "        self.conv1d_1 = nn.Conv1d(in_channels=input_dim, out_channels=hidden_dim, kernel_size=1, padding=0, bias = True)\n",
        "        self.conv1d_2 = nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=1, padding=0, bias = True)\n",
        "        self.conv1d_3 = nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=1, padding=0, bias = True)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=1, padding=0)\n",
        "        self.lstm_1 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True, bias = True)\n",
        "        self.lstm_2 = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, bidirectional=True, bias = True)\n",
        "        self.attention_linear = nn.Linear(hidden_dim * 2, 1, bias = True)\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim, bias = True)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: (batch_size, sequence_length, input_dim)\n",
        "        batch_size, sequence_length, input_dim = x.size()\n",
        "\n",
        "        # Reshape for Conv1d input\n",
        "        x = x.permute(0, 2, 1)  # Reshape for Conv1d input\n",
        "        x = self.pool(F.relu(self.conv1d_1(x)))\n",
        "        x = self.pool(F.relu(self.conv1d_2(x)))\n",
        "        x = self.pool(F.relu(self.conv1d_3(x)))\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        # Reshape for LSTM input\n",
        "        x = x.permute(0, 2, 1)  # Reshape for LSTM input\n",
        "        x, _ = self.lstm_1(x)\n",
        "        x, _ = self.lstm_2(x)\n",
        "        x = self.dropout(x)\n",
        "        # Attention mechanism\n",
        "        attention_weights = F.softmax(self.attention_linear(x), dim=1)\n",
        "        attended_out = torch.sum(attention_weights * x, dim=1)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.linear(attended_out)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W6HmvfCW_xY"
      },
      "source": [
        "# Model Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHouVk-FXPsf"
      },
      "outputs": [],
      "source": [
        "def model_run(model,train_loader,test_loader,val_loader,model_details,work_dir):\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  num_epochs = 100\n",
        "  all_predictions = []\n",
        "  all_true_values = []\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  # Training\n",
        "  #best_val_loss = float('inf')\n",
        "  patience = 10  # Number of epochs to wait for improvement\n",
        "  counter = 0\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      running_train_loss = 0.0\n",
        "      progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False, mininterval=1)\n",
        "      for inputs, labels in progress_bar:\n",
        "          inputs, labels = inputs.to(device), labels.to(device)\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(inputs.float())\n",
        "          loss = criterion(outputs.squeeze(), labels.float())\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_train_loss += loss.item()\n",
        "          progress_bar.set_postfix({'loss': running_train_loss / len(train_loader)})\n",
        "\n",
        "      # Record training loss for this epoch\n",
        "      train_losses.append(running_train_loss / len(train_loader))\n",
        "\n",
        "      # Validation loop\n",
        "      model.eval()\n",
        "      running_val_loss = 0.0\n",
        "      predictions = []\n",
        "      true_values = []\n",
        "      with torch.no_grad():\n",
        "      #    val_progress_bar = tqdm(val_dataloader, desc=f'Validation', leave=False, mininterval=1)\n",
        "          for inputs, labels in val_loader:\n",
        "              inputs, labels = inputs.to(device), labels.to(device)#labels are normalized\n",
        "\n",
        "              # Forward pass\n",
        "              normalized_outputs = model(inputs.float())\n",
        "              loss = criterion(normalized_outputs.squeeze(), labels.float()) # losses are computed between normalized outputs and nromalized prediction\n",
        "\n",
        "              running_val_loss += loss.item()\n",
        "\n",
        "              #outputs = normalized_outputs * vector_max[-1]\n",
        "              #labels = labels * vector_max[-1]\n",
        "              # Store predicted and true values\n",
        "              predictions.extend(normalized_outputs.cpu().numpy())\n",
        "              true_values.extend(labels.cpu().numpy())\n",
        "\n",
        "          # Record validation loss for this epoch\n",
        "          val_losses.append(running_val_loss / len(val_loader))\n",
        "\n",
        "      all_predictions.append(predictions)\n",
        "      all_true_values.append(true_values)\n",
        "\n",
        "      #TestingModel\n",
        "      model.eval()\n",
        "      running_test_loss = 0.0\n",
        "      predictions_test_data = []\n",
        "      true_values_test_data = []\n",
        "      with torch.no_grad():\n",
        "      #    val_progress_bar = tqdm(val_dataloader, desc=f'Validation', leave=False, mininterval=1)\n",
        "          for inputs, labels in test_loader:\n",
        "              inputs, labels = inputs.to(device), labels.to(device)#labels are normalized\n",
        "\n",
        "              # Forward pass\n",
        "              normalized_outputs = model(inputs.float())\n",
        "              loss = criterion(normalized_outputs.squeeze(), labels.float()) # losses are computed between normalized outputs and nromalized prediction\n",
        "\n",
        "              running_val_loss += loss.item()\n",
        "\n",
        "              #outputs = normalized_outputs * vector_max[-1]\n",
        "              #labels = labels * vector_max[-1]\n",
        "              # Store predicted and true values\n",
        "              predictions_test_data.extend(normalized_outputs.cpu().numpy())\n",
        "              true_values_test_data.extend(labels.cpu().numpy())\n",
        "\n",
        "          print(f\"Test Loss for {model_details}:\", running_val_loss / len(test_loader))\n",
        "      \"\"\"\n",
        "      avg_val_loss = running_val_loss / len(val_dataloader)\n",
        "      if avg_val_loss < best_val_loss: #check for the improvement of val_loss\n",
        "          best_val_loss = avg_val_loss\n",
        "          counter = 0\n",
        "      else:\n",
        "          counter = counter + 1\n",
        "      if counter >= patience:\n",
        "          print(f'Early stopping after {epoch+1} epochs without improvement.')\n",
        "          break\"\"\"\n",
        "      # Print average loss for this epoch\n",
        "      print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}\")\n",
        "\n",
        "  # Plot training and validation losses\n",
        "  plt.figure()\n",
        "  plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
        "  plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title(f'Training and Validation Losses for {model_details}')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  plt.savefig(f'{work_dir}/model_loss_{model_details}.png')\n",
        "\n",
        "  #TestingModel\n",
        "  model.eval()\n",
        "  running_test_loss = 0.0\n",
        "  predictions_test_data = []\n",
        "  true_values_test_data = []\n",
        "  with torch.no_grad():\n",
        "  #    val_progress_bar = tqdm(val_dataloader, desc=f'Validation', leave=False, mininterval=1)\n",
        "      for inputs, labels in test_loader:\n",
        "          inputs, labels = inputs.to(device), labels.to(device)#labels are normalized\n",
        "\n",
        "          # Forward pass\n",
        "          normalized_outputs = model(inputs.float())\n",
        "          loss = criterion(normalized_outputs.squeeze(), labels.float()) # losses are computed between normalized outputs and nromalized prediction\n",
        "\n",
        "          running_val_loss += loss.item()\n",
        "\n",
        "          #outputs = normalized_outputs * vector_max[-1]\n",
        "          #labels = labels * vector_max[-1]\n",
        "          # Store predicted and true values\n",
        "          predictions_test_data.extend(normalized_outputs.cpu().numpy())\n",
        "          true_values_test_data.extend(labels.cpu().numpy())\n",
        "\n",
        "      print(f\"Test Loss for {model_details}:\", running_val_loss / len(test_loader))\n",
        "\n",
        "  return predictions_test_data,true_values_test_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8JSK08yZh2q"
      },
      "source": [
        "# Post Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUEBqW0GZsnQ"
      },
      "outputs": [],
      "source": [
        "def post_processing(test_mean_ref,test_std_ref,predictions_test_data,true_values_test_data,sequence_length,model_details,work_dir, companies):\n",
        "  test_mean_ref_target = []\n",
        "  test_std_ref_target = []\n",
        "\n",
        "  for i in range(len(test_mean_ref)):\n",
        "    test_mean_ref_target.append(test_mean_ref[i][-1])\n",
        "    test_std_ref_target.append(test_std_ref[i][-1])\n",
        "\n",
        "  test_by_comp_pred=[]\n",
        "  test_by_comp_act=[]\n",
        "  test_by_comp_pred = [x[0] for x in predictions_test_data]\n",
        "\n",
        "  test_by_comp_pred = [test_by_comp_pred[i * (10-sequence_length) : (i + 1) * (10-sequence_length)] for i in range(stockno)]\n",
        "  test_by_comp_act = [true_values_test_data[i * (10-sequence_length) : (i + 1) * (10-sequence_length)] for i in range(stockno)]\n",
        "\n",
        "  for i in range(len(test_by_comp_pred)):\n",
        "    test_by_comp_pred[i] = [x * test_std_ref_target[i] + test_mean_ref_target[i] for x in test_by_comp_pred[i]]\n",
        "    test_by_comp_act[i] = [x * test_std_ref_target[i] + test_mean_ref_target[i] for x in test_by_comp_act[i]]\n",
        "\n",
        "\n",
        "  pct_value_error = {}\n",
        "  maerr = {}\n",
        "  for i in range(len(test_by_comp_act)):\n",
        "    pred_tmp = test_by_comp_pred[i]\n",
        "    act_tmp = test_by_comp_act[i]\n",
        "    pct_value_error[companies[i]] = mean_absolute_percentage_error(act_tmp,pred_tmp)\n",
        "    maerr[companies[i]] = mean_absolute_error(act_tmp,pred_tmp)\n",
        "\n",
        "\n",
        "  with open(f'{work_dir}/mae_{model_details}.json', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(maerr))\n",
        "\n",
        "  with open(f'{work_dir}/pct_{model_details}.json', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(pct_value_error))\n",
        "\n",
        "  predictions={}\n",
        "  actual={}\n",
        "\n",
        "  for i in range(len(test_by_comp_pred)):\n",
        "    predictions[companies[i]]=test_by_comp_pred[i]\n",
        "    actual[companies[i]]=test_by_comp_act[i]\n",
        "\n",
        "  with open(f'{work_dir}/pred_{model_details}.json', 'w') as convert_file:\n",
        "     convert_file.write(json.dumps(predictions))\n",
        "\n",
        "  with open(f'{work_dir}/act_{model_details}.json', 'w') as convert_file:\n",
        "      convert_file.write(json.dumps(actual))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNh9jjJcbAPS"
      },
      "source": [
        "# Experiment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nL6Q0GWbFII"
      },
      "outputs": [],
      "source": [
        "sequence_length = 4\n",
        "overlap = 3\n",
        "batch_size = 25\n",
        "input_dim = len(data[0]) - 1\n",
        "hidden_dim = 32\n",
        "output_dim = 1\n",
        "dropout_prob = 0.2\n",
        "\n",
        "\n",
        "#initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "MLP_model = MLP(input_dim, output_dim, sequence_length).to(device)\n",
        "LSTM_model = LSTM(input_dim, 2, output_dim).to(device)\n",
        "LSTM_AT_model = LSTMAttentionRegressor(input_dim, 2, output_dim).to(device)\n",
        "BiLSTM_AM_model = BiLSTM_AM(input_dim, hidden_dim, output_dim, sequence_length, dropout_prob).to(device)\n",
        "CNN_BiLSTM_AM_model = CNN_BiLSTM_AM(input_dim, hidden_dim, output_dim, sequence_length, dropout_prob).to(device)\n",
        "\n",
        "models={\n",
        "        'MLP':MLP_model,\n",
        "        'LSTM':LSTM_model,\n",
        "        'LSTM_AT':LSTM_AT_model,\n",
        "        'BiLSTM_AM':BiLSTM_AM_model,\n",
        "        'CNN_LSTM_AM':CNN_BiLSTM_AM_model}\n",
        "\n",
        "# Model implementation\n",
        "for model_name,model in models.items():\n",
        "  print(\"Model:\",model_name)\n",
        "  train_loader,val_loader,test_loader,test_mean_ref,test_std_ref = pre_process(data,sequence_length,overlap,batch_size)\n",
        "  predictions_test_data,true_values_test_data = model_run(model,train_loader,test_loader,val_loader,model_name,result_dir)\n",
        "  post_processing(test_mean_ref,test_std_ref,predictions_test_data,true_values_test_data,sequence_length,model_name,result_dir, companies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wuq0USTvJj4h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "AGqjvbxSUDmu",
        "96xhX_0mUPBJ",
        "F43LdsZ8UJlE",
        "BD89p_PWUXHm",
        "BH3nuEeyVnAi",
        "5W6HmvfCW_xY",
        "H8JSK08yZh2q",
        "xNh9jjJcbAPS"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
